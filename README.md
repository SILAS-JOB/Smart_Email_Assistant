Assistente Inteligente de E-mails üìß‚ú®

Aplica√ß√£o web que utiliza Intelig√™ncia Artificial para classificar e-mails, otimizar o fluxo de trabalho e aumentar a produtividade. O sistema categoriza mensagens como "Produtivo" ou "Improdutivo" e sugere respostas autom√°ticas adequadas ao contexto.

Este projeto foi desenvolvido como o case pr√°tico para o processo seletivo de Desenvolvedor da AutoU.

üöÄ Acesse a Aplica√ß√£o ao Vivo: [COLOQUE AQUI O LINK DO SEU APP NO RENDER.COM]

üé¨ Demonstra√ß√£o

A interface permite a an√°lise de texto inserido diretamente ou atrav√©s do upload de arquivos .txt e .pdf. O resultado exibe a categoria, a confian√ßa da IA e uma resposta sugerida.

[ADICIONE AQUI UM SCREENSHOT OU UM GIF DA SUA APLICA√á√ÉO FUNCIONANDO]

üìã √çndice

    Sobre o Projeto

    Funcionalidades

    Tecnologias Utilizadas

    Arquitetura e Decis√µes T√©cnicas

    Como Rodar o Projeto Localmente

    Deploy

    Licen√ßa

üìñ Sobre o Projeto

O objetivo deste desafio foi desenvolver uma solu√ß√£o para uma empresa do setor financeiro que lida com um alto volume de e-mails. A aplica√ß√£o automatiza a leitura e triagem, liberando a equipe de tarefas manuais e repetitivas, permitindo que foquem em atividades de maior valor.

O sistema utiliza modelos de Processamento de Linguagem Natural (NLP) para entender o conte√∫do dos e-mails e realizar duas a√ß√µes principais:

    Classifica√ß√£o Zero-Shot: Determina se o e-mail requer uma a√ß√£o (Produtivo) ou n√£o (Improdutivo).

    Gera√ß√£o de Texto: Cria uma sugest√£o de resposta em portugu√™s, adaptada √† categoria do e-mail.

‚ú® Funcionalidades

    ‚úÖ Classifica√ß√£o de E-mails: An√°lise de texto para categoriza√ß√£o autom√°tica.

    ‚úÖ Sugest√£o de Respostas: Gera√ß√£o de respostas contextuais com IA.

    ‚úÖ Input Flex√≠vel: Suporte para inser√ß√£o de texto direto e upload de arquivos (.txt e .pdf).

    ‚úÖ Exibi√ß√£o de Confian√ßa: Mostra a porcentagem de confian√ßa do modelo na classifica√ß√£o.

    ‚úÖ Interface Intuitiva: Feedback visual de "carregando" e bot√£o para copiar a resposta gerada.

    ‚úÖ Pr√©-processamento de Texto: Utiliza√ß√£o de NLTK para limpeza de stopwords antes da an√°lise.

üõ†Ô∏è Tecnologias Utilizadas

    Backend: Python 3, Flask, Gunicorn

    Intelig√™ncia Artificial:

        Hugging Face (Inference API)

        Biblioteca huggingface_hub para comunica√ß√£o robusta com a API

    Frontend: HTML5, CSS3, JavaScript (ES6)

    Processamento de Arquivos: PyPDF2

    Gerenciamento de Depend√™ncias: Pip, Venv

    Deploy: Render

üß† Arquitetura e Decis√µes T√©cnicas

A aplica√ß√£o foi estruturada seguindo o padr√£o Application Factory em Flask para garantir organiza√ß√£o e escalabilidade, com uma clara separa√ß√£o de responsabilidades entre rotas, servi√ßos e templates.

O principal desafio t√©cnico foi estabelecer uma comunica√ß√£o est√°vel e correta com a Inference API da Hugging Face. A jornada de depura√ß√£o foi um aprendizado valioso:

    Diagn√≥stico de Erros: As tentativas iniciais com requests manuais resultaram em erros 404 Not Found persistentes, mesmo com identificadores de modelo aparentemente v√°lidos.

    Identifica√ß√£o da Causa Raiz: Atrav√©s de logs detalhados e da an√°lise de erros 402 Payment Required (ap√≥s esgotar a cota gratuita de testes), ficou claro que a comunica√ß√£o precisava ser mais robusta. O erro final, Model is not supported for task text-generation. Supported task: conversational, revelou que os modelos mais modernos exigem um tipo de tarefa espec√≠fico.

    Solu√ß√£o Profissional: A decis√£o estrat√©gica foi abandonar a abordagem manual e adotar a biblioteca oficial huggingface_hub. Utilizando o InferenceClient, foi poss√≠vel interagir com os modelos atrav√©s das fun√ß√µes corretas (zero_shot_classification e chat_completion), o que resolveu definitivamente os problemas de conex√£o.

    Melhoria de Precis√£o: Para aumentar a acur√°cia da classifica√ß√£o de textos em portugu√™s, foi implementada a t√©cnica de Few-Shot Prompting, fornecendo exemplos diretamente no prompt enviado ao modelo, o que melhorou significativamente seu racioc√≠nio contextual.

‚öôÔ∏è Como Rodar o Projeto Localmente

Siga os passos abaixo para configurar e executar a aplica√ß√£o em seu ambiente local.

    Clone o reposit√≥rio:
    Bash

git clone [COLOQUE AQUI A URL DO SEU REPOSIT√ìRIO GIT]
cd [NOME-DA-PASTA-DO-PROJETO]

Crie e ative um ambiente virtual:
Bash

# No Windows
python -m venv venv
.\venv\Scripts\activate

# No macOS/Linux
python3 -m venv venv
source venv/bin/activate

Instale as depend√™ncias:

pip install -r requirements.txt

Configure as vari√°veis de ambiente:

    Crie um arquivo chamado .env na raiz do projeto.

    Adicione sua chave de API da Hugging Face dentro dele:

    HUGGING_FACE_API_KEY="hf_sua_chave_secreta_aqui"

Execute a aplica√ß√£o:
Bash

    python run.py

    A aplica√ß√£o estar√° dispon√≠vel em http://127.0.0.1:5000.

‚òÅÔ∏è Deploy

A aplica√ß√£o est√° hospedada na plataforma Render, configurada para deploy cont√≠nuo a partir da branch main do reposit√≥rio. O servidor de produ√ß√£o utilizado √© o Gunicorn, e as chaves de API s√£o gerenciadas como vari√°veis de ambiente no painel do Render para garantir a seguran√ßa.

üìÑ Licen√ßa

Este projeto est√° sob a licen√ßa MIT. Veja o arquivo LICENSE para mais detalhes.
